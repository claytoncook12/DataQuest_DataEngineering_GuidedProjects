{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Guided Project: Hacker News Pipeline\n",
    "\n",
    "In this course, we began with the concepts of functional programming, and then built our own data pipeline class in Python. We learned about advanced Python concepts such as the decorators, closures, and good API design. In the last mission, we also learned how to implement a directed acyclic graph (DAG) as the scheduler for our pipeline.\n",
    "\n",
    "After completing all these missions, we have finally built a robust data pipeline that schedules our tasks in the correct order! In this guided project, we will use the pipeline we have been building, and apply it to a real world data pipeline project. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "To make things easier, we have already downloaded a list of JSON posts to a file called <code>hn_stories_2014.json</code>. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "<code>created_at</code>: A timestamp of the story's creation time.<br>\n",
    "<code>created_at_i</code>: A unix epoch timestamp.<br>\n",
    "<code>url</code>: The URL of the story link.<br>\n",
    "<code>objectID</code>: The ID of the story.<br>\n",
    "<code>author</code>: The story's author (username on HN).<br>\n",
    "<code>points</code>: The number of upvotes the story had.<br>\n",
    "<code>title</code>: The headline of the post.<br>\n",
    "<code>num_comments</code>: The number of a comments a post has.<br>\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our <code>Pipeline</code> class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
    "\n",
    "We have provided a solution to the guided project for you. You can find it in this [link](https://github.com/dataquestio/solutions/blob/master/Mission267Solutions.ipynb)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pipeline import Pipeline\n",
    "from pipeline import build_csv\n",
    "import io\n",
    "import csv\n",
    "import string\n",
    "from stop_words import stop_words\n",
    "from collections import Counter"
   ]
  },
  {
   "source": [
    "We'll start the project by loading the JSON file data into Python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Class\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    \"\"\"\n",
    "    Input string or file object of a json file and returns list of \"stories\"\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(pipeline.f, str):\n",
    "        pipeline.f = open(pipeline.f, mode='r')\n",
    "    \n",
    "    j = json.loads(pipeline.f.read())\n",
    "    \n",
    "    return j['stories']\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(f):\n",
    "    \"\"\"\n",
    "    Filter stories\n",
    "    \"\"\"\n",
    "    for line in f:\n",
    "        created_at = line['created_at']\n",
    "        created_at_i = line['created_at_i']\n",
    "        url = line['url']\n",
    "        objectID = line['objectID']\n",
    "        author = line['author']\n",
    "        points = int(line['points'])\n",
    "        title = line['title']\n",
    "        num_comments = int(line['num_comments'])\n",
    "        # Filter conditions\n",
    "        if points > 50 and num_comments > 1 and \"Ask HN\" not in str(line['title']):\n",
    "            yield(\n",
    "                objectID, created_at, url, points, title\n",
    "            )\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(f):\n",
    "    \"\"\"\n",
    "    Creates CSV of filtered data\n",
    "    \"\"\"\n",
    "\n",
    "    # Created file information\n",
    "    header = ['objectID','created_at','url','points','title']\n",
    "    io_file = io.StringIO()\n",
    "\n",
    "    return build_csv(f,header=header,file=io_file)\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(io_file):\n",
    "    \"\"\"\n",
    "    Returns a generator of every Hacker News Story title\n",
    "    \"\"\"\n",
    "\n",
    "    reader = csv.reader(io_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "\n",
    "    return(line[idx] for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(f):\n",
    "    \"\"\"\n",
    "    Returns generator of cleaned titles\n",
    "    \"\"\"\n",
    "\n",
    "    def replace_char(t):\n",
    "        \"\"\"\n",
    "        Removes punctuation and lowers case of string\n",
    "        \"\"\"\n",
    "        for char in string.punctuation:\n",
    "            if char in t:\n",
    "                t = t.replace(char,\"\")\n",
    "        return t.lower()\n",
    "\n",
    "    return (replace_char(x) for x in f)\n",
    "\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keywords_dictonary(f):\n",
    "    \"\"\"\n",
    "    Returns Counter object of words in cleaned titles\n",
    "    \"\"\"\n",
    "\n",
    "    word_freq = Counter()\n",
    "\n",
    "    for words in f:\n",
    "        for word in words.split(\" \"):\n",
    "            word_freq[word] += 1\n",
    "\n",
    "    for word in stop_words:\n",
    "        del word_freq[word]\n",
    "    \n",
    "    del word_freq['']\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "@pipeline.task(depends_on=build_keywords_dictonary)\n",
    "def top_100(f):\n",
    "    \"\"\"\n",
    "    Returns Tuple of (Key,Count) of Top 100 Most Common Words\n",
    "    \"\"\"\n",
    "\n",
    "    return tuple(f.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(('new', 185),\n",
       " ('google', 167),\n",
       " ('bitcoin', 101),\n",
       " ('open', 92),\n",
       " ('programming', 90),\n",
       " ('web', 88),\n",
       " ('data', 85),\n",
       " ('video', 79),\n",
       " ('python', 75),\n",
       " ('code', 72),\n",
       " ('facebook', 71),\n",
       " ('released', 71),\n",
       " ('using', 70),\n",
       " ('2013', 65),\n",
       " ('javascript', 65),\n",
       " ('free', 64),\n",
       " ('source', 64),\n",
       " ('game', 63),\n",
       " ('internet', 62),\n",
       " ('microsoft', 59),\n",
       " ('c', 59),\n",
       " ('linux', 58),\n",
       " ('app', 57),\n",
       " ('pdf', 55),\n",
       " ('work', 54),\n",
       " ('language', 54),\n",
       " ('software', 52),\n",
       " ('2014', 52),\n",
       " ('startup', 51),\n",
       " ('apple', 50),\n",
       " ('use', 50),\n",
       " ('make', 50),\n",
       " ('time', 48),\n",
       " ('yc', 48),\n",
       " ('security', 48),\n",
       " ('nsa', 45),\n",
       " ('github', 45),\n",
       " ('windows', 44),\n",
       " ('world', 41),\n",
       " ('way', 41),\n",
       " ('like', 41),\n",
       " ('1', 40),\n",
       " ('project', 40),\n",
       " ('computer', 40),\n",
       " ('heartbleed', 40),\n",
       " ('git', 37),\n",
       " ('users', 37),\n",
       " ('dont', 37),\n",
       " ('design', 37),\n",
       " ('ios', 37),\n",
       " ('developer', 36),\n",
       " ('os', 36),\n",
       " ('twitter', 36),\n",
       " ('ceo', 36),\n",
       " ('vs', 36),\n",
       " ('life', 36),\n",
       " ('big', 35),\n",
       " ('day', 35),\n",
       " ('android', 34),\n",
       " ('online', 34),\n",
       " ('years', 33),\n",
       " ('simple', 33),\n",
       " ('court', 33),\n",
       " ('guide', 32),\n",
       " ('learning', 32),\n",
       " ('mt', 32),\n",
       " ('api', 32),\n",
       " ('says', 32),\n",
       " ('apps', 32),\n",
       " ('browser', 32),\n",
       " ('server', 31),\n",
       " ('firefox', 31),\n",
       " ('fast', 31),\n",
       " ('gox', 31),\n",
       " ('problem', 31),\n",
       " ('mozilla', 31),\n",
       " ('engine', 31),\n",
       " ('site', 31),\n",
       " ('introducing', 30),\n",
       " ('amazon', 30),\n",
       " ('year', 30),\n",
       " ('support', 29),\n",
       " ('stop', 29),\n",
       " ('built', 29),\n",
       " ('better', 29),\n",
       " ('million', 29),\n",
       " ('people', 29),\n",
       " ('text', 29),\n",
       " ('3', 28),\n",
       " ('does', 28),\n",
       " ('tech', 28),\n",
       " ('development', 28),\n",
       " ('billion', 27),\n",
       " ('developers', 27),\n",
       " ('just', 27),\n",
       " ('library', 27),\n",
       " ('did', 27),\n",
       " ('website', 27),\n",
       " ('money', 27),\n",
       " ('inside', 27))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "f = open('hn_stories_2014.json')\n",
    "pipeline.f = f #set pipeline start file \n",
    "output = pipeline.run()\n",
    "output[top_100]"
   ]
  }
 ]
}